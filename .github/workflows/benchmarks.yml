name: benchmarks

on:
  pull_request:
    branches: [master]
  workflow_dispatch:

permissions:
  contents: read

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5  # v4

      - name: Set up Python 3.12
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5
        with:
          python-version: '3.12'

      - name: Install uv
        uses: astral-sh/setup-uv@e58605a9b6da7c637471fab8847a5e5a6b8df081  # v5

      - name: Set up project
        run: uv sync --group dev

      - name: Run benchmarks
        run: |
          uv run pytest -m benchmark --benchmark-enable \
            --benchmark-json=benchmark-results.json \
            --benchmark-columns=mean,stddev,rounds

      - name: Upload benchmark results
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
        with:
          name: benchmark-results
          path: benchmark-results.json

      - name: Check for regressions
        run: |
          if [ -f .benchmarks/baseline.json ]; then
            uv run pytest-benchmark compare \
              .benchmarks/baseline.json benchmark-results.json \
              --csv=comparison.csv
          else
            echo "No baseline found â€” skipping regression check"
          fi
