name: benchmarks

on:
  pull_request:
    branches: [master]
  workflow_dispatch:

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3

      - name: Set up Python 3.12
        uses: actions/setup-python@e9aba2c848f5ebd159c070c61ea2c4e2b122355e # v2
        with:
          python-version: '3.12'

      - name: Install uv
        uses: astral-sh/setup-uv@caf0cab7a618c569241d31dcd442f54681755d39 # v3
        with:
          version: "latest"

      - name: Set up project
        run: uv sync --dev

      - name: Run benchmarks
        run: |
          uv run pytest -m benchmark --benchmark-enable \
            --benchmark-json=benchmark-results.json \
            --benchmark-columns=mean,stddev,rounds

      - name: Upload benchmark results
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
        with:
          name: benchmark-results
          path: benchmark-results.json

      - name: Check for regressions
        run: |
          if [ -f .benchmarks/baseline.json ]; then
            uv run pytest-benchmark compare \
              .benchmarks/baseline.json benchmark-results.json \
              --csv=comparison.csv
          else
            echo "No baseline found â€” skipping regression check"
          fi
